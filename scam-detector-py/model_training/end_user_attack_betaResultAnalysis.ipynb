{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from web3 import Web3\n",
    "from hexbytes import HexBytes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rlp\n",
    "import requests\n",
    "from web3 import Web3\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import logging\n",
    "import typing\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from web3 import Web3\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.INFO)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "root.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_labels() -> pd.DataFrame:\n",
    "        df_forta = pd.DataFrame(columns=['createdAt', 'id', 'label', 'source'])\n",
    "        return df_forta\n",
    "\n",
    "def labels_by_source_id(source_id: str, start_date: datetime, end_date: datetime) -> pd.DataFrame:\n",
    "    url = \"https://api.forta.network/graphql\"\n",
    "    chunk_size = 8000\n",
    "\n",
    "    df_forta = empty_labels()\n",
    "    json_data = \"\"\n",
    "    first_run = True\n",
    "    count = 0\n",
    "    while (json_data == \"\" or json_data['data']['labels']['pageInfo']['hasNextPage']):\n",
    "        query = \"\"\"query exampleQuery {\n",
    "                    labels(\n",
    "                        input: {\n",
    "                            SOURCEIDS_CLAUSE\n",
    "                            CREATEDBEFORE_CLAUSE\n",
    "                            CREATEDSINCE_CLAUSE\n",
    "                            AFTER_CLAUSE\n",
    "                            CHUNKSIZE\n",
    "                            state: true\n",
    "                        }\n",
    "                    ) {\n",
    "                        pageInfo {\n",
    "                            endCursor {\n",
    "                                pageToken\n",
    "                            }\n",
    "                            hasNextPage\n",
    "                        }\n",
    "                        labels {\n",
    "                            createdAt\n",
    "                            id\n",
    "                            label {\n",
    "                                label\n",
    "                                metadata\n",
    "                                remove\n",
    "                                entityType\n",
    "                                entity\n",
    "                                confidence\n",
    "                            }\n",
    "                            source {\n",
    "                                chainId\n",
    "                                \n",
    "                                alertId\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\"\"\"\n",
    "\n",
    "        after_clause = \"\"\n",
    "        if(first_run is False):\n",
    "            pageToken = json_data['data']['labels']['pageInfo']['endCursor']['pageToken']\n",
    "            after_clause = \"\"\"after: {{pageToken:\"{0}\"}}\"\"\".format(pageToken)\n",
    "\n",
    "        # this is a bit hacky\n",
    "        query = query.replace(\"SOURCEIDS_CLAUSE\", f\"\"\"sourceIds: [\"{source_id}\"]\"\"\")\n",
    "        query = query.replace(\"CREATEDBEFORE_CLAUSE\", f\"\"\"createdBefore: {int(end_date.timestamp()*1000)}\"\"\")\n",
    "        query = query.replace(\"CREATEDSINCE_CLAUSE\", f\"\"\"createdSince: {int(start_date.timestamp()*1000)}\"\"\")\n",
    "        query = query.replace(\"AFTER_CLAUSE\", after_clause)\n",
    "        query = query.replace(\"CHUNKSIZE\", f\"first: {chunk_size}\") \n",
    "\n",
    "        #print(query)\n",
    "\n",
    "        retries = 1\n",
    "        wait = 1\n",
    "        success = False\n",
    "        while not success:\n",
    "            try:\n",
    "                count += 1\n",
    "                r = requests.post(url, json={'query': query})\n",
    "                if r.status_code == 200:\n",
    "                    success = True\n",
    "                    if chunk_size < 5000:\n",
    "                        chunk_size *= 2\n",
    "                        logging.warning(f\"Increasing chunk size to {chunk_size}\")\n",
    "                else:\n",
    "                    raise Exception(f\"status code: {r.status_code} {r.text}\")\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Unable to retrieve alerts {r.status_code} , {e}\")\n",
    "                logging.warning(f\"Sleeping {wait}sec. Count {count}.\")\n",
    "                old_chunk_size = chunk_size\n",
    "                chunk_size = int(chunk_size/2)\n",
    "                if(chunk_size<1):\n",
    "                    chunk_size = 1\n",
    "                query = query.replace(f\"first: {old_chunk_size},\", f\"first: {chunk_size},\") \n",
    "                logging.warning(f\"Reducing chunk size to {chunk_size}\")\n",
    "                time.sleep(wait)\n",
    "                retries += 1\n",
    "                if retries > 30:\n",
    "                    raise Exception(\"Unable to retrieve alerts even after repeated retries. Pls check logs\")\n",
    "\n",
    "        json_data = json.loads(r.text)\n",
    "        df_data = json_data['data']['labels']['labels']\n",
    "        df_forta = pd.concat([pd.DataFrame(df_data), df_forta])\n",
    "\n",
    "        first_run = False\n",
    "        count += 1\n",
    "\n",
    "    return df_forta\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime(2023, 6, 14, 0, 0, 0, 0, timezone.utc)\n",
    "end_date = datetime(2023, 6, 22, 0, 0, 0, 0, timezone.utc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_id = \"0x47c45816807d2eac30ba88745bf2778b61bc106bc76411b520a5289495c76db8\"\n",
    "\n",
    "file_name = f\"ScamDetector_{source_id}_labels_{start_date.strftime('%Y%m%d')}-{end_date.strftime('%Y%m%d')}.csv\"\n",
    "\n",
    "df_forta_labels = empty_labels() # 'createdAt', 'id', 'label', 'source'\n",
    "df_forta_labels = labels_by_source_id(source_id, start_date, end_date)\n",
    "\n",
    "\n",
    "# for index, row in df_forta_labels.iterrows():\n",
    "#     created_date = row['createdAt']\n",
    "#     source = row['source']\n",
    "#     alert_id = source['alertId']\n",
    "#     label = row['label']\n",
    "#     entity = label['entity']\n",
    "#     entityType = label['entityType']\n",
    "#     label_tag = label['label']\n",
    "#     remove = label['remove']\n",
    "#     confidence = label['confidence']\n",
    "#     metadata = label['metadata']\n",
    "#     handler_type = \"\"\n",
    "#     if 'handlerType' in metadata:\n",
    "#         handler_type = metadata['handlerType']\n",
    "#     chain_id = source['chainId']\n",
    "#     df_temp_labels = pd.DataFrame(columns=['createdAt', 'entity', 'entityType', 'label', 'chain_id', 'confidence', 'remove', 'alert_id', 'handler_type', 'metadata'], data=[[created_date, entity, entityType, label_tag, chain_id, confidence, remove, alert_id, handler_type, metadata]])\n",
    "#     df_labels = pd.concat([df_labels, df_temp_labels])\n",
    "\n",
    "\n",
    "\n",
    "# df_labels['createdAt'] = pd.to_datetime(df_labels['createdAt'])  # Ensure the 'date' column is in datetime format\n",
    "# df_sorted = df_labels.sort_values(by='createdAt')  # Sort DataFrame by 'date' column\n",
    "# df_labels_deduplicated = df_sorted.drop_duplicates(subset=['entity', 'chain_id', 'hanlder_type'], keep='first')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_labels_deduplicated.reset_index(inplace=True)\n",
    "\n",
    "# df_labels_deduplicated.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(items: list, key: str):\n",
    "    v = ''\n",
    "    for item in items:\n",
    "        if item.startswith(key):\n",
    "            v = item.split('=')[1]\n",
    "            break\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'createdAt', 'id', 'label', 'source'\n",
    "df_forta_labels['createdAt'] = pd.to_datetime(df_forta_labels['createdAt'])\n",
    "df_forta_labels['chainId'] = df_forta_labels['source'].apply(lambda x: x['chainId'])\n",
    "df_forta_labels['labelstr'] = df_forta_labels['label'].apply(lambda x: x['label'])\n",
    "df_forta_labels['handlerType'] = df_forta_labels['labelstr'].apply(lambda x: x.split('/')[2] if '/' in x else x)\n",
    "df_forta_labels['threatCategory'] = df_forta_labels['labelstr'].apply(lambda x: x.split('/')[1] if '/' in x else x)\n",
    "df_forta_labels['labelstr'] = df_forta_labels['labelstr'].apply(lambda x: x.split('/')[0] if '/' in x else x)\n",
    "df_forta_labels['entity'] = df_forta_labels['label'].apply(lambda x: x['entity'])\n",
    "df_forta_labels['entityType'] = df_forta_labels['label'].apply(lambda x: x['entityType'])\n",
    "df_forta_labels['remove'] = df_forta_labels['label'].apply(lambda x: x['remove'])\n",
    "df_forta_labels['confidence'] = df_forta_labels['label'].apply(lambda x: x['confidence'])\n",
    "df_forta_labels['metadata'] = df_forta_labels['label'].apply(lambda x: x['metadata'])\n",
    "df_forta_labels['botVersion'] = df_forta_labels['label'].apply(lambda x: get_value(x['metadata'], 'bot_version'))\n",
    "df_forta_labels['addressType'] = df_forta_labels['label'].apply(lambda x: get_value(x['metadata'], 'address_type'))\n",
    "df_forta_labels['handlerType2'] = df_forta_labels['label'].apply(lambda x: get_value(x['metadata'], 'handler_type'))\n",
    "df_forta_labels['handlerType3'] = df_forta_labels['label'].apply(lambda x: get_value(x['metadata'], 'logic'))\n",
    "df_forta_labels['threatCategory2'] = df_forta_labels['label'].apply(lambda x: get_value(x['metadata'], 'threat_category'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forta_labels.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ScamDetector_0x47c45816807d2eac30ba88745bf2778b61bc106bc76411b520a5289495c76db8_labels_20230614-20230622.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ethereum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
